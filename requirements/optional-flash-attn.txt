# Flash Attention for efficient attention computation
# Install with: pip install -r requirements/optional-flash-attn.txt --no-build-isolation
flash-attn>=2.5.0
