# Example configuration for using the new grpo_weighted_mul advantage method
# This uses MULTIPLICATION operation: combined_advantages = grpo_advantages * weights

algorithm:
  # Use the new grpo_weighted_mul advantage estimator
  adv_estimator: grpo_weighted_mul  # Options: 'gae', 'grpo', 'grpo_weighted_add', 'grpo_weighted_mul'
  
  # Enable GPT-4 based solution classification
  enable_solution_classification: true  # Set to false to disable GPT-4 classification
  
  # Standard PPO parameters
  gamma: 1.0
  lam: 1.0
  kl_penalty: "kl"
  kl_ctrl:
    type: "adaptive"
    kl_coef: 0.2
    target_kl: 0.01
    horizon: 10000

trainer:
  total_epochs: 1
  project_name: "grpo_weighted_mul_experiment"
  experiment_name: "math_reasoning_with_diversity_mul"
  default_local_dir: "./outputs/grpo_weighted_mul"
  save_freq: 100
  test_freq: 50
  nnodes: 1
  n_gpus_per_node: 2
  critic_warmup: 0
  remove_previous_ckpt: true
  logger: "wandb"

data:
  train_files: ["path/to/your/train.parquet"]
  val_files: ["path/to/your/val.parquet"]
  prompt_key: "prompt"
  max_prompt_length: 512
  train_batch_size: 8
  val_batch_size: 8
  shuffle: true
  seed: 42

actor_rollout_ref:
  hybrid_engine: true
  rollout:
    n: 4  # Number of responses per prompt (important for grpo_weighted_mul)
  actor:
    strategy: "fsdp"
    ppo_mini_batch_size: 8
    ppo_micro_batch_size_per_gpu: 4
    use_dynamic_bsz: false
    optim:
      type: "adamw"
      lr: 1e-5
      weight_decay: 0.01
  ref:
    strategy: "fsdp"
    log_prob_micro_batch_size_per_gpu: 4
    use_dynamic_bsz: false
  model:
    path: "microsoft/DialoGPT-medium"  # Replace with your model path
    use_remove_padding: true

critic:
  strategy: "fsdp"
  ppo_mini_batch_size: 8
  ppo_micro_batch_size_per_gpu: 4
  use_dynamic_bsz: false
  optim:
    type: "adamw"
    lr: 1e-5
    weight_decay: 0.01

reward_model:
  enable: false  # Set to true if using a reward model

# Notes:
# 1. The grpo_weighted_mul method requires multiple responses per prompt (rollout.n > 1)
# 2. GPT-4 classification adds computational overhead but improves diversity weighting
# 3. Set enable_solution_classification: false for faster training without classification
# 4. This version uses MULTIPLICATION instead of ADDITION for combining advantages and weights 